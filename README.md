# ambiguity_calibration

In real world settings, ambiguity is constantly present. There is inherent ambiguity in language, dialogue between humans, and realworld tasks. For example, we may ask a human to "Go meet my friend at Starbucks". How do we know which Starbucks we should visit? Furthermore, an important step in achieving general artificial intelligence is human-level language understanding in autonomous agents or robots. Currently, the most likely candidates for endowing these systems with such knowledge are large language models (LLMs) like GPT-3. Thus, it is important to investigate how LLMs handle ambiguity and whether they do it in a manner that makes sense to humans. In this project, we conduct two experiments containing many tasks of varying complexity and diversity that aims to shed light into how LLMs handle ambiguity. Our results show that LLMs do handle ambiguity in a consistent and coherent manner on certain tasks, although different LLMs exhibit different behaviors regardless of the task.

The paper is found above titled ambig_cal.pdf
